
<html>

<head>
<style type="text/css">
.knitr .inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage .left {
  text-align: left;
}
.rimage .right {
  text-align: right;
}
.rimage .center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>Practical Machine Learning Course Project</title>
</head>

<body>

<!-- <font face="Verdana" size="2"/> -->
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'/>
<font face="Open Sans"/>

<h1>Practical Machine Learning Course Project</h1>

<p>The goal of the project was to use a machine learning algorithm with sensor data generated by test subjects lifting weights to classify the lift that generated a particular set of sample data. Appropriate classifications were A for lifts executed with good technique versus B, C, D or E, for lifts executed with poor technique (where each of these classifications reflects a specific technical error). Training data were labeled accordingly and provided alongside unlabeled test data (which, as directed, was not used in this write-up).</p>

<h2>Dependencies</h2>

<p>The <a href="http://topepo.github.io/caret/index.html">caret</a> package was used for its data splitting and training functionality, loaded as follows:</p>

<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(caret)</span>
</pre></div>
</div></div>

<h2>Loading and Splitting Data</h2>

<p>In order to get a good prediction of accuracy,  and since caret would fit the parameters of the model using k-fold cross validation, 30% of the training data was set aside. The training data were loaded and split as follows:</p>

<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">dataRaw</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">read.csv</span><span class="hl std">(</span><span class="hl str">&quot;pml-training.csv&quot;</span><span class="hl std">)</span>
<span class="hl std">trainingIndices</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">createDataPartition</span><span class="hl std">(dataRaw</span><span class="hl opt">$</span><span class="hl std">classe,</span> <span class="hl kwc">p</span><span class="hl std">=</span><span class="hl num">0.7</span><span class="hl std">,</span> <span class="hl kwc">list</span><span class="hl std">=F)</span>
<span class="hl std">training</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainingRaw[trainingIndices,]</span>
<span class="hl std">testing</span> <span class="hl kwb">&lt;-</span> <span class="hl std">trainingRaw[</span><span class="hl opt">-</span><span class="hl std">trainingIndices,]</span>
</pre></div>
</div></div>

<h2>Data Cleaning</h2>

<p>As can be seen, the data contained a large number of columns (160), not all of them likely to be useful predictors:</p>

<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">str</span><span class="hl std">(training,</span> <span class="hl kwc">list.len</span><span class="hl std">=</span><span class="hl num">20</span><span class="hl std">)</span>
</pre></div>
<div class="output"><pre class="knitr r">## 'data.frame':	13737 obs. of  160 variables:
##  $ X                       : int  1 2 3 5 6 8 9 11 13 15 ...
##  $ user_name               : Factor w/ 6 levels &quot;adelmo&quot;,&quot;carlitos&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
##  $ raw_timestamp_part_2    : int  788290 808298 820366 196328 304277 440390 484323 500302 560359 604281 ...
##  $ cvtd_timestamp          : Factor w/ 20 levels &quot;2/12/2011 13:32&quot;,..: 15 15 15 15 15 15 15 15 15 15 ...
##  $ new_window              : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...
##  $ roll_belt               : num  1.41 1.41 1.42 1.48 1.45 1.42 1.43 1.45 1.42 1.45 ...
##  $ pitch_belt              : num  8.07 8.07 8.07 8.07 8.06 8.13 8.16 8.18 8.2 8.2 ...
##  $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
##  $ total_accel_belt        : int  3 3 3 3 3 3 3 3 3 3 ...
##  $ kurtosis_roll_belt      : Factor w/ 397 levels &quot;&quot;,&quot;-0.01685&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ kurtosis_picth_belt     : Factor w/ 317 levels &quot;&quot;,&quot;-0.021887&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ kurtosis_yaw_belt       : Factor w/ 2 levels &quot;&quot;,&quot;#DIV/0!&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ skewness_roll_belt      : Factor w/ 395 levels &quot;&quot;,&quot;-0.003095&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ skewness_roll_belt.1    : Factor w/ 338 levels &quot;&quot;,&quot;-0.005928&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ skewness_yaw_belt       : Factor w/ 2 levels &quot;&quot;,&quot;#DIV/0!&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ max_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ max_picth_belt          : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ max_yaw_belt            : Factor w/ 68 levels &quot;&quot;,&quot;-0.1&quot;,&quot;-0.2&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##   [list output truncated]
</pre></div>
</div></div>


<p>Inspection showed many of these columns had issues, as follows:</p>
<ul>
     <li>Fields unrelated to classification, such as the lifter's name. The documentation stated that each lifter performed lifts of each classification for 10 reps (which was confirmed by visual inspection of the data), thus the lifter name would not be a helpful predictor and was removed. </li>
     <li>Fields that accurately predicted the lift classification on the training set (and indeed would on the testing set, since the testing set was sourced from the same pool) but would not generalize. Several time fields featured in the data, which proved to be excellent predictors (since the lifters were instructed in every case to perform 10 reps of A followed by 10 reps of B and so forth). Since these data would not generalize to future data and to avoid 'cheating' these values were removed.</li>
     <li>Fields (sometimes) present in training data but not in the testing data. Clearly these would not be useful predictors and so they were removed.</li>
     <li>Fields that were always or frequently NA were removed.</li>
</ul>

<p>The removal of the columns, provided the further benefit of reducing the number of columns to 57, which would reduce training time significantly:</p>

<div class="chunk" id="unnamed-chunk-4"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">cleanedColumns</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">8</span><span class="hl opt">:</span><span class="hl num">11</span><span class="hl std">,</span> <span class="hl num">37</span><span class="hl opt">:</span><span class="hl num">49</span><span class="hl std">,</span> <span class="hl num">60</span><span class="hl opt">:</span><span class="hl num">68</span><span class="hl std">,</span> <span class="hl num">84</span><span class="hl opt">:</span><span class="hl num">86</span><span class="hl std">,</span> <span class="hl num">102</span><span class="hl std">,</span> <span class="hl num">113</span><span class="hl opt">:</span><span class="hl num">124</span><span class="hl std">,</span> <span class="hl num">140</span><span class="hl std">,</span> <span class="hl num">151</span><span class="hl opt">:</span><span class="hl num">159</span><span class="hl std">,</span> <span class="hl num">160</span><span class="hl std">)</span>
<span class="hl std">trainingCleaned</span> <span class="hl kwb">&lt;-</span> <span class="hl std">training[, cleanedColumns]</span>
<span class="hl std">testingCleaned</span> <span class="hl kwb">&lt;-</span> <span class="hl std">testing[, cleanedColumns]</span>
<span class="hl kwd">names</span><span class="hl std">(trainingCleaned)</span>
</pre></div>
<div class="output"><pre class="knitr r">##  [1] &quot;roll_belt&quot;            &quot;pitch_belt&quot;           &quot;yaw_belt&quot;            
##  [4] &quot;total_accel_belt&quot;     &quot;gyros_belt_x&quot;         &quot;gyros_belt_y&quot;        
##  [7] &quot;gyros_belt_z&quot;         &quot;accel_belt_x&quot;         &quot;accel_belt_y&quot;        
## [10] &quot;accel_belt_z&quot;         &quot;magnet_belt_x&quot;        &quot;magnet_belt_y&quot;       
## [13] &quot;magnet_belt_z&quot;        &quot;roll_arm&quot;             &quot;pitch_arm&quot;           
## [16] &quot;yaw_arm&quot;              &quot;total_accel_arm&quot;      &quot;gyros_arm_x&quot;         
## [19] &quot;gyros_arm_y&quot;          &quot;gyros_arm_z&quot;          &quot;accel_arm_x&quot;         
## [22] &quot;accel_arm_y&quot;          &quot;accel_arm_z&quot;          &quot;magnet_arm_x&quot;        
## [25] &quot;magnet_arm_y&quot;         &quot;magnet_arm_z&quot;         &quot;roll_dumbbell&quot;       
## [28] &quot;pitch_dumbbell&quot;       &quot;yaw_dumbbell&quot;         &quot;total_accel_dumbbell&quot;
## [31] &quot;gyros_dumbbell_x&quot;     &quot;gyros_dumbbell_y&quot;     &quot;gyros_dumbbell_z&quot;    
## [34] &quot;accel_dumbbell_x&quot;     &quot;accel_dumbbell_y&quot;     &quot;accel_dumbbell_z&quot;    
## [37] &quot;magnet_dumbbell_x&quot;    &quot;magnet_dumbbell_y&quot;    &quot;magnet_dumbbell_z&quot;   
## [40] &quot;roll_forearm&quot;         &quot;pitch_forearm&quot;        &quot;yaw_forearm&quot;         
## [43] &quot;total_accel_forearm&quot;  &quot;gyros_forearm_x&quot;      &quot;gyros_forearm_y&quot;     
## [46] &quot;gyros_forearm_z&quot;      &quot;accel_forearm_x&quot;      &quot;accel_forearm_y&quot;     
## [49] &quot;accel_forearm_z&quot;      &quot;magnet_forearm_x&quot;     &quot;magnet_forearm_y&quot;    
## [52] &quot;magnet_forearm_z&quot;     &quot;classe&quot;
</pre></div>
</div></div>

<h2>Building the Model</h2>

<p>A Random Forest classifier was used for its versatility, non-linearity and transparency (it is fairly easy to see and debug the rules underlying the trained classifier). If this had not proved to provide the requisite accuracy, SVMs, Boosting and Linear Discriminant Analysis would have been tested and perhaps some subset of classifiers used in an ensemble, but this did not prove necessary.</p>

<p>The model was trained as follows, its time being recorded for interest and in case it would prove useful data later (e.g. in ensembling). Note that the seed is set to enable the results to be reproduced later if necessary:</p>

<div class="chunk" id="unnamed-chunk-5"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">424242</span><span class="hl std">)</span>
<span class="hl std">trainingTime</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">system.time</span><span class="hl std">(model</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">train</span><span class="hl std">(classe</span> <span class="hl opt">~</span><span class="hl std">.,</span> <span class="hl kwc">data</span><span class="hl std">=trainingCleaned,</span> <span class="hl kwc">method</span><span class="hl std">=</span><span class="hl str">&quot;rf&quot;</span><span class="hl std">,</span> <span class="hl kwc">PROX</span><span class="hl std">=T))</span>
</pre></div>
</div></div>

<p>As can be seen, the model was time-consuming to train (running on an Asus Zenbook 301, with a dual core Intel i7 @ 2.8 Ghz, with 8 GB of RAM):</p>

<div class="chunk" id="unnamed-chunk-6"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">trainingTime[</span><span class="hl str">&quot;elapsed&quot;</span><span class="hl std">]</span>
</pre></div>
</div></div>
<div class="chunk" id="unnamed-chunk-7"><div class="rcode"><div class="output"><pre class="knitr r">## elapsed 
## 4460.48
</pre></div>
</div></div>

<h2>Inspecting the Model</h2>

<p>As mentioned above, it is possible to see in detail the rules employed by the forest. In particular, we can see the relative importance (i.e. predictive power) given to each variable using the varImp function.</p>

<div class="chunk" id="unnamed-chunk-8"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">varImp</span><span class="hl std">(model)</span>
</pre></div>
</div></div>
<div class="chunk" id="unnamed-chunk-9"><div class="rcode"><div class="output"><pre class="knitr r">## rf variable importance
## 
##   only 20 most important variables shown (out of 52)
## 
##                   Overall
## roll_belt          100.00
## yaw_belt            77.83
## magnet_dumbbell_z   71.97
## magnet_dumbbell_y   65.27
## pitch_forearm       64.62
## pitch_belt          64.51
## roll_forearm        53.17
## magnet_dumbbell_x   52.59
## accel_belt_z        46.45
## magnet_belt_z       45.66
## roll_dumbbell       45.19
## accel_dumbbell_y    44.51
## magnet_belt_y       41.41
## accel_dumbbell_z    40.28
## roll_arm            34.63
## accel_forearm_x     32.45
## accel_dumbbell_x    31.95
## gyros_belt_z        31.22
## yaw_dumbbell        30.73
## gyros_dumbbell_y    28.91
</pre></div>
</div></div>

<h2>Model Accuracy</h2>

<p>To determine whether the model was sufficient, its accuracy in cross-validation was examined:</p>

<div class="chunk" id="unnamed-chunk-10"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">model</span>
</pre></div>
</div></div>
<div class="chunk" id="unnamed-chunk-11"><div class="rcode"><div class="output"><pre class="knitr r">## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## 
## Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9885667  0.9855308  0.001924787  0.002438536
##   27    0.9885024  0.9854502  0.001845531  0.002336228
##   52    0.9784945  0.9727870  0.003951176  0.004998504
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.
</pre></div>
</div></div>

<p>The resulting 99% accuracy was deemed sufficient and so this model was selected.</p>

<p>Finally, to project the out of sample accuracy of the model, it was run against the set held out at the beginning:</p>

<div class="chunk" id="unnamed-chunk-12"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">predictions</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">predict</span><span class="hl std">(model,</span> <span class="hl kwc">newdata</span><span class="hl std">=trainingCleanedFairestValidation)</span>
<span class="hl kwd">confusionMatrix</span><span class="hl std">(predictions, trainingCleanedFairestValidation</span><span class="hl opt">$</span><span class="hl std">classe)</span>
</pre></div>
</div></div>
<div class="chunk" id="unnamed-chunk-13"><div class="rcode"><div class="output"><pre class="knitr r">## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    8    0    0    0
##          B    0 1130   22    0    0
##          C    0    1 1003   14    3
##          D    0    0    1  948    0
##          E    0    0    0    2 1079
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9913          
##                  95% CI : (0.9886, 0.9935)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.989           
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9921   0.9776   0.9834   0.9972
## Specificity            0.9981   0.9954   0.9963   0.9998   0.9996
## Pos Pred Value         0.9952   0.9809   0.9824   0.9989   0.9981
## Neg Pred Value         1.0000   0.9981   0.9953   0.9968   0.9994
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2845   0.1920   0.1704   0.1611   0.1833
## Detection Prevalence   0.2858   0.1958   0.1735   0.1613   0.1837
## Balanced Accuracy      0.9991   0.9937   0.9869   0.9916   0.9984
</pre></div>
</div></div>

<h2>Summary</h2>

<p>The above has shown how, after pre-processing of the provided data, a Random Forest classifier was trained and shown to achieve 99% accuracy in its predictions.</p>

<font>
</body>
</html>
